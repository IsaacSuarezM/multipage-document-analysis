# Multipage Document Analysis with Generative AI

Este repositorio contiene una soluci√≥n completa para el an√°lisis automatizado de documentos PDF de m√∫ltiples p√°ginas utilizando Inteligencia Artificial Generativa (GenAI) y Amazon Bedrock.

## üéØ Descripci√≥n

Esta soluci√≥n permite extraer informaci√≥n definida por el usuario de documentos PDF arbitrariamente largos, superando las limitaciones de ventana de contexto de los LLMs mediante una t√©cnica similar a map-reduce. La soluci√≥n utiliza los modelos Claude 3 de Anthropic disponibles en Amazon Bedrock.

## üèóÔ∏è Arquitectura

La soluci√≥n implementa un enfoque map-reduce para procesar documentos largos:

1. **Map**: Extrae informaci√≥n de chunks del documento menores a la ventana de contexto del LLM
2. **Reduce**: Consolida las m√∫ltiples instancias de informaci√≥n extra√≠da en un √∫nico resultado

![Arquitectura](readme_assets/architecture.png)

## üöÄ Servicios AWS Utilizados

- **Amazon Textract**: Extracci√≥n de texto de PDFs
- **Amazon Bedrock**: Modelos Claude 3 Haiku y Claude 3.5 Sonnet
- **AWS Lambda**: Funciones de procesamiento
- **Amazon DynamoDB**: Almacenamiento de resultados
- **Amazon S3**: Almacenamiento de documentos
- **Amazon API Gateway**: API REST
- **Amazon Cognito**: Autenticaci√≥n de usuarios
- **AWS Step Functions**: Orquestaci√≥n del workflow
- **Amazon SNS/SQS**: Notificaciones y colas
- **Amazon EventBridge**: Triggers de eventos

## üìã Prerrequisitos

### Software requerido:
- **Node.js** >= 14.0.0
- **Python** >= 3.10
- **Docker**
- **AWS CLI** configurado
- **AWS CDK** >= v2.174.1

### Permisos AWS:
- Acceso a los modelos **Claude 3 Haiku** y **Claude 3.5 Sonnet V1** en Amazon Bedrock
- Permisos para crear recursos de infraestructura (IAM, Lambda, DynamoDB, etc.)

## üõ†Ô∏è Instalaci√≥n y Despliegue

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio-url>
cd multipage-document-analysis
```

### 2. Desplegar el Backend

```bash
cd backend

# Crear entorno virtual
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate.bat

# Instalar dependencias
pip install -r requirements.txt

# Login en ECR p√∫blico
aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws

# Bootstrap CDK (si es necesario)
cdk bootstrap

# Desplegar
cdk deploy \
--parameters LanguageCode=es \
--parameters IncludeExamples=false \
--parameters PagesChunk=5 \
--parameters ExtractionConfidenceLevel=85 \
--require-approval never
```

### 3. Desplegar el Frontend

```bash
cd ../frontend
# Seguir las instrucciones en frontend/README.md
```

## ‚öôÔ∏è Par√°metros de Configuraci√≥n

| Par√°metro | Descripci√≥n | Valores | Por defecto |
|-----------|-------------|---------|-------------|
| `LanguageCode` | Idioma del documento y resultados | `es`, `en` | `es` |
| `IncludeExamples` | Usar few-shot learning | `true`, `false` | `false` |
| `PagesChunk` | P√°ginas por chunk | N√∫mero entero | `5` |
| `ExtractionConfidenceLevel` | Umbral de confianza (0-99) | 0-99 | `85` |

## üí∞ Costos Estimados

**Aproximadamente $1,273 USD/mes** para procesar 1,000 documentos de 100 p√°ginas cada uno.

### Desglose de costos (regi√≥n us-east-1):
- Amazon Textract: ~$150
- Amazon Bedrock (Claude 3 Haiku): ~$938
- Amazon Bedrock (Claude 3.5 Sonnet): ~$150
- Otros servicios AWS: ~$35

## üìä Outputs del Stack

Despu√©s del despliegue, obtendr√°s:

- **ApiGatewayRestApiEndpoint**: URL de la API REST
- **CognitoUserPoolId**: ID del User Pool de Cognito
- **CognitoUserPoolClientId**: ID del cliente de aplicaci√≥n
- **CognitoIdentityPoolId**: ID del Identity Pool
- **RegionName**: Regi√≥n de despliegue

## üîß Personalizaci√≥n

### Definir informaci√≥n a extraer

1. Navega a `backend/pace_backend/text_analysis_workflow/shared/`
2. Crea `InformationExtraction.py` siguiendo el ejemplo de `CharterReports.py`
3. Define objetos Pydantic para la informaci√≥n a extraer
4. Actualiza `section_definition.py` con tus nuevas secciones

### Agregar ejemplos (Few-shot learning)

1. Crea ejemplos en `backend/pace_backend/text_analysis_workflow/extract_data_to_schema_fn/prompt_selector/examples/{language_code}/`
2. Para cada secci√≥n, agrega pares de archivos:
   - `example_chunk_{i}.txt`: Texto de ejemplo
   - `example_chunk_{i}.json`: Informaci√≥n extra√≠da correspondiente

## üîí Seguridad

- Autenticaci√≥n mediante Amazon Cognito
- Protecci√≥n API con AWS WAF
- Cifrado en tr√°nsito y en reposo
- Principio de menor privilegio en roles IAM

## üìö Documentaci√≥n Adicional

- [README del Backend](backend/README.md)
- [README del Frontend](frontend/README.md)
- [Gu√≠as de Seguridad](readme_assets/security.md)

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver el archivo [LICENSE](LICENSE) para m√°s detalles.

## ‚ö†Ô∏è Disclaimer Legal

El c√≥digo de ejemplo, bibliotecas de software, herramientas de l√≠nea de comandos, pruebas de concepto, plantillas u otra tecnolog√≠a relacionada se proporciona como "Contenido de AWS" bajo el Acuerdo de Cliente de AWS. No debes usar este Contenido de AWS en tus cuentas de producci√≥n, o en datos de producci√≥n u otros datos cr√≠ticos.

## üìû Soporte

Para preguntas o problemas:

1. Revisa la documentaci√≥n
2. Busca en los issues existentes
3. Crea un nuevo issue con detalles del problema

---

**Nota**: Este proyecto requiere acceso a los modelos de Amazon Bedrock y puede incurrir en costos de AWS. Revisa los costos estimados antes del despliegue.
